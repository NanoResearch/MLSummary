\relax 
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Comparison between ML Techniques}}{4}}
\newlabel{SummaryTable}{{1}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Choosing Unsupervised Learning Algorithms}}{4}}
\newlabel{UnsupervisedSummary}{{2}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Supervised Learning: (Multiple) Linear Regression}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Gradient Descent}{5}}
\newlabel{EqnLMS}{{3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Gradient Descent Implementation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Feature Scaling}{7}}
\newlabel{EqnNormalization}{{4}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Normal Equation Method}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Probabilistic Interpretation}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Locally weighted linear regression}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Supervised Learning: Classification and Logistic Regression}{8}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Supervised Learning: Support Vector Machine}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Large Margin Intuition}{9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Mathematics Behind Large Margin Classification}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Kernels}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Using An SVM}{10}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Supervised Learning: Neural Networks}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Neural Networks: Representation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Neural Networks: Learning}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Backpropagation Algorithm}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Random Initialization}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Putting it all together}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Reinforcement Learning: Recommender Systems}{13}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Large-Scale Machine Learning}{15}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Application Example: Optical Character Recognition in Photographs}{16}}
\@writefile{toc}{\contentsline {section}{\numberline {9}General Linear Models}{17}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Generative Learning Algorithms}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.0.4}Multivariate Gaussian (normal distribution)}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.1}Gaussian Discriminant Analysis}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.2}Generative and Discriminative comparison}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {10.3}Naive Bayes}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.1}Laplace Smoothing}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {10.3.2}Event Models}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Support Vector Machine Theory}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.1}Margins}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.1.1}Functional Margins}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.1.2}Geometric Margin}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.2}Optimal Margin Classifier}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.1}Lagrange duality}{21}}
\newlabel{KKT Complementarity}{{60}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {11.2.2}Using the dual on optimal margin classifiers}{22}}
\newlabel{LagangianOptimization1}{{65}{22}}
\newlabel{LagrangianOptimalw}{{67}{23}}
\newlabel{LagrangianOptimalb}{{68}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.3}Kernels}{23}}
\@writefile{toc}{\contentsline {paragraph}{Mercer's Theorem}{24}}
\@writefile{toc}{\contentsline {paragraph}{The Gaussian Kernel}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.4}Regularization and the non-separable case}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {11.5}Sequential Minimal Optimization}{25}}
\@writefile{toc}{\contentsline {paragraph}{Coordinate Ascent}{25}}
\@writefile{toc}{\contentsline {paragraph}{Sequential Minimal Optimization}{25}}
\@writefile{toc}{\contentsline {section}{\numberline {12}Learning Theory}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.1}Bias (under-fitting) and Variance (over-fitting)}{26}}
\@writefile{toc}{\contentsline {paragraph}{Union Bound Lemma}{27}}
\@writefile{toc}{\contentsline {paragraph}{Hoeffding inequality, or Chernoff bound}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.2}Finite hypothesis classes}{27}}
\newlabel{UniformConvergence}{{107}{27}}
\@writefile{toc}{\contentsline {paragraph}{Theorem}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {12.3}Infinite hypothesis classes}{28}}
\@writefile{toc}{\contentsline {paragraph}{Theorem (Vapnik)}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {13}Regularization and model selection}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.1}Cross validation}{29}}
\@writefile{toc}{\contentsline {paragraph}{$k$-fold cross validation}{29}}
\@writefile{toc}{\contentsline {paragraph}{Leave-one-out cross validation}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.2}Feature selection}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.3}Bayesian statistics and regularization}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {13.4}Regularization}{31}}
\@writefile{toc}{\contentsline {section}{\numberline {14}Online Learning}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {14.1}The Perceptron}{32}}
\@writefile{toc}{\contentsline {section}{\numberline {15}Machine Learning Application and System Design}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.1}Debugging learning algorithms}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.2}Learning Curves: Bias and variance}{32}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.3}Algorithm vs. objective}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {15.4}Machine Learning System Design}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.4.1}Error Analysis}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.4.2}Ablative analysis}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.4.3}Skewed classes: Precision and Recall}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {15.4.4}Confusion Matrices}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {16}Unsupervised Learning: $k$-means clustering}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {17}Unsupervised Learning: Mixture of Gaussians and the Expectation-Maximization Algorithm}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.1}EM on the Mixture of Gaussians model}{36}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.2}The General EM Algorithm}{38}}
\@writefile{toc}{\contentsline {paragraph}{Jensen's inequality (Theorem):}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {17.2.1}Revisiting Mixture of Gaussians}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {17.2.2}Mixture of Naive Bayes model}{40}}
\@writefile{toc}{\contentsline {subsection}{\numberline {17.3}Anomaly detection}{40}}
\@writefile{toc}{\contentsline {section}{\numberline {18}Unsupervised Learning: Factor Analysis}{41}}
\newlabel{FactorAnalysisModel1}{{177}{41}}
\newlabel{FactorAnalysisModel2}{{174}{41}}
\@writefile{toc}{\contentsline {subsection}{\numberline {18.1}EM algorithm for Factor Analysis}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {19}Unsupervised Learning: Principal Component Analysis}{42}}
\@writefile{toc}{\contentsline {section}{\numberline {20}Unsupervised Learning: Independent Component Analysis}{44}}
\@writefile{toc}{\contentsline {section}{\numberline {21}Reinforcement Learning}{45}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.1}Markov Decision Process (MDP)}{45}}
\newlabel{OptimalPolicy}{{211}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.2}Value Iteration Algorithm}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.3}Policy Iteration Algorithm}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {21.4}Dealing with continuous state spaces: Value Function Approximation}{47}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {21.4.1}Fitted value iteration}{48}}
